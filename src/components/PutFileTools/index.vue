<script setup>
import SparkMD5, { hash } from 'spark-md5'
import { ref } from 'vue'
import axios from 'axios'
import UploadQueue from './uploadQueue.js'

const props = defineProps({
  // åˆ‡ç‰‡å¤§å°
  chunkSize: {
    type: Number,
    default: 10 * 1024 * 1024, // 10MB
  },
  /**
   * @description æ ¡éªŒæ–‡ä»¶æ˜¯å¦åœ¨æœåŠ¡å™¨ä¸­å­˜åœ¨
   * @return {Boolean | Array} å¦‚æœåœ¨æœåŠ¡å™¨ä¸­ä¸Šä¼ äº†éƒ¨åˆ†åˆ‡ç‰‡åˆ™è¿”å›è¿™äº›åˆ‡ç‰‡çš„hash
   */
  inspectRequest: {
    type: Function,
    default: '',
  },
  // æ ¡éªŒæ˜¯å¦å·²ç»ä¸Šä¼ åˆ°æœåŠ¡çš„æ¥å£åœ°å€
  inspectApiUrl: {
    type: String,
    default: '',
  },
  // ä¸Šä¼ æ–‡ä»¶çš„æ¥å£åœ°å€
  uploadApiUrl: {
    type: String,
    default: '',
  },
  // åˆ‡ç‰‡ä¸Šä¼ çš„å¹¶å‘æ•°
  concurrencyNumber: {
    type: Number,
    default: 3,
  },
})

const emits = defineEmits([
  'onUploadProgress',
  'uploadError',
  'onChange',
  'onUploadFinish',
  'onFinish'
])

let blobSlice = null
let file = null
// æ•´ä¸ªæ–‡ä»¶çš„hash
let computedHash = ''
// æ–‡ä»¶å¤§å°
let fileSize = 0
// åˆ‡ç‰‡æ•°
let chunkNumber = 0
// å½“å‰å¤„ç†çš„åˆ‡ç‰‡ç´¢å¼•
let currentChunkIndex = 0
// å½“å‰ä¸Šä¼ çš„åˆ‡ç‰‡ç´¢å¼•
let currentUploadChunkIndex = 0
// æ˜¯å¦å·²ç»åœ¨æœåŠ¡å™¨å­˜åœ¨
let isLoaded = false
let fileReader = null
// ä¸Šä¼ è¿›åº¦
let progress = 0
// åˆ‡ç‰‡åˆ—è¡¨ä¿¡æ¯
let chunklist = []
// å·²ä¸Šä¼ åˆ—è¡¨
let uploadedChunkList = []
let spark = null
// å·²ä¸Šä¼ å¤§å°
let uploadedSize = 0
let uploadQueue = new UploadQueue(props.concurrencyNumber)

function init() {
  computedHash = ''
  fileSize = 0
  uploadedSize = 0
  progress = 0
  isLoaded = false
  chunkNumber = 0
  currentChunkIndex = 0
  currentUploadChunkIndex = 0
  file = null
  uploadedChunkList = []
}

async function onchange(e) {
  init()
  blobSlice =
    File.prototype.slice ||
    File.prototype.mozSlice ||
    File.prototype.webkitSlice
  file = e.target.files[0]
  emits('onChange', file)
  fileProcessing()
}

// æŠŠæ–‡ä»¶å¤„ç†æˆåˆ‡ç‰‡
function fileProcessing() {
  fileSize = file.size
  chunkNumber = Math.ceil(file.size / props.chunkSize)
  spark = new SparkMD5.ArrayBuffer()
  fileReader = new FileReader()
  currentChunkIndex = 0

  fileReader.onload = async function (e) {
    const chunkFormData = new FormData()
    // åˆ‡ç‰‡çš„hash
    let chunkHash = SparkMD5.ArrayBuffer.hash(e.target.result)
    console.log("ğŸš€ ~ file: index.vue:108 ~ e.target.result:", e.target.result)

    spark.append(e.target.result)
    // åˆ‡ç‰‡æ–‡ä»¶
    chunkFormData.append('chunk', new Blob([e.target.result]))
    // åˆ‡ç‰‡æ–‡ä»¶hash
    chunkFormData.append('chunkHash', chunkHash)
    const fileUpload = fileUploadRequest(chunkFormData)
    uploadQueue.add(fileUpload)
    chunklist.push({
      chunkHash,
      formData:chunkFormData
    })
    currentChunkIndex++
    if (currentChunkIndex < chunkNumber) {
      loadNext()
    } else {
      computedHash = spark.end()
      isLoaded = await inspectRequest()
      cheakChunkUpload()
    }
  }

  fileReader.onerror = function () {
    console.warn('oops, something went wrong.')
  }
  loadNext()
}

// è¯»å–æ–‡ä»¶åˆ‡ç‰‡
function loadNext() {
  const start = currentChunkIndex * props.chunkSize
  const end =
    start + props.chunkSize >= file.size ? file.size : start + props.chunkSize
  // æŒ‰å­—èŠ‚è¯»å–æ–‡ä»¶å†…å®¹
  fileReader.readAsArrayBuffer(blobSlice.call(file, start, end))
}

// æ ¡éªŒæ–‡ä»¶æ˜¯å¦åœ¨æœåŠ¡å™¨ä¸­å­˜åœ¨
async function inspectRequest() {
  if (props.inspectRequest) {
    const data = await props.inspectRequest(computedHash,file)
    return data
  } else {
    return false
  }
}

// ä¸Šä¼ è¯·æ±‚
function fileUploadRequest(chunkFormData) {
  return function () {
    return new Promise((resolve) => {
      chunkFormData.append('hash', computedHash)
      axios({
        method: 'POST',
        url: props.uploadApiUrl,
        data: chunkFormData,
        headers: {
          hash: computedHash,
          chunkHash: chunkFormData.get('chunkHash'),
        },
        onUploadProgress: function (progressEvent) {
          progress = progressEvent.loaded / fileSize
          emits('onUploadProgress', progress)
        },
      })
        .then((res) => {
          uploadedChunkList.push(chunkFormData.get('chunkHash'))
          resolve(res,chunkFormData)
        })
        .catch((error) => {
          emits('uploadError', error)
        })
    })
  }
}

// åˆ‡ç‰‡ä¸Šä¼ 
async function chunkUpload() {
  const uploadChunkQueue = uploadQueue.concurrencyQueue()
  uploadChunkQueue.forEach((item) => {
    item().then((res) => {
      uploadQueueShift()
    })
  })
}

function uploadQueueShift() {
  console.log("ğŸš€ ~ file: index.vue:200 ~ uploadQueueShift ~ chunklist:", chunklist)
    console.log("ğŸš€ ~ file: index.vue:200 ~ uploadQueueShift ~ uploadedChunkList:", uploadedChunkList)
  console.log("ğŸš€ ~ file: index.vue:199 ~ uploadQueueShift ~ uploadQueue:", uploadQueue.queueLength())

  if(uploadQueue.queueLength() === 0 && uploadedChunkList.length === chunklist.length) {

    emits('onFinish',computedHash)
    return
  }
  uploadQueue
    .shift()()
    .then((res) => {
      uploadQueueShift()
    })
}

async function cheakChunkUpload() {
  if (isLoaded === true) {
    // éªŒè¯æ–‡ä»¶æ˜¯å¦å·²ç»åœ¨æœåŠ¡ç«¯å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨ï¼Œé‚£å°±ä¸ç”¨ä¸Šä¼ äº†ï¼Œç›¸å½“äºç§’ä¼ æˆåŠŸã€‚
    progress = 100
    emits('onUploadProgress', progress)
  } else {
    if (isLoaded === false) {
      chunkUpload()
    } else if (Object.prototype.toString.call(isLoaded) === '[object Array]') {
    }
  }
}
</script>

<template>
  <div class="greetings">
    <input type="file" @change="onchange" />
  </div>
</template>
